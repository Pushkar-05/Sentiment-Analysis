{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqHolBuaV6vE",
        "outputId": "ac96d501-7c74-4770-80c0-f2d80dda537e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1]\n",
            "True Labels: [0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1]\n",
            "Accuracy: 0.85\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Create pseudo (synthetic) data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 2)  # 100 samples, 2 features\n",
        "\n",
        "# 2. Create labels (binary classification based on a simple rule)\n",
        "# if x1 + x2 > 1 => label 1, else 0\n",
        "y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Train a machine learning model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# 6. Evaluate\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"True Labels:\", y_test)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Data\n",
        "texts = [\n",
        "    \"AI is amazing\",\n",
        "    \"I hate this medicine\",\n",
        "    \"The robot works perfectly\",\n",
        "    \"This diet doesn't work\",\n",
        "    \"Robotics is the future\",\n",
        "    \"This product is terrible\",\n",
        "    \"This surgery helped a lot\",\n",
        "    \"I love machine learning\"\n",
        "]\n",
        "\n",
        "# Labels\n",
        "# Sentiment: 1 = positive, 0 = negative\n",
        "# Topic:     1 = tech,     0 = health\n",
        "sentiments = [1, 0, 1, 0, 1, 0, 1, 1]\n",
        "topics     = [1, 0, 1, 0, 1, 0, 0, 1]\n",
        "\n",
        "# 2. Vectorize\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# 3. Train-test split\n",
        "y = list(zip(sentiments, topics))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# 4. Model\n",
        "model = MultiOutputClassifier(LogisticRegression())\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict\n",
        "predictions = model.predict(X_test)\n",
        "y_test_sentiments, y_test_topics = zip(*y_test)\n",
        "pred_sentiments, pred_topics = zip(*predictions)\n",
        "\n",
        "# 6. Evaluation\n",
        "print(\"=== Sentiment Classification ===\")\n",
        "print(classification_report(y_test_sentiments, pred_sentiments, zero_division=0))\n",
        "\n",
        "print(\"=== Topic Classification ===\")\n",
        "print(classification_report(y_test_topics, pred_topics, zero_division=0))\n",
        "\n",
        "# 7. Predict on New Text\n",
        "new_text = [\"This robot is bad\"]\n",
        "X_new = vectorizer.transform(new_text)\n",
        "pred_sentiment, pred_topic = model.predict(X_new)[0]\n",
        "\n",
        "# 8. Decode labels\n",
        "sentiment_map = {0: \"Negative\", 1: \"Positive\"}\n",
        "topic_map = {0: \"Health\", 1: \"Tech\"}\n",
        "\n",
        "print(\"\\n=== New Text Prediction ===\")\n",
        "print(\"Text:\", new_text[0])\n",
        "print(\"Sentiment:\", sentiment_map[pred_sentiment])\n",
        "print(\"Topic    :\", topic_map[pred_topic])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDWFZqbSXSH7",
        "outputId": "47461e4e-8a82-47b3-b34d-f8a15780a17a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Sentiment Classification ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       2.0\n",
            "           1       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00       2.0\n",
            "   macro avg       0.00      0.00      0.00       2.0\n",
            "weighted avg       0.00      0.00      0.00       2.0\n",
            "\n",
            "=== Topic Classification ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       2.0\n",
            "           1       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00       2.0\n",
            "   macro avg       0.00      0.00      0.00       2.0\n",
            "weighted avg       0.00      0.00      0.00       2.0\n",
            "\n",
            "\n",
            "=== New Text Prediction ===\n",
            "Text: This robot is bad\n",
            "Sentiment: Positive\n",
            "Topic    : Tech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your new input text\n",
        "new_text = [\"This robot is useless\"]\n",
        "\n",
        "# Vectorize it using the same vectorizer\n",
        "X_new = vectorizer.transform(new_text)\n",
        "\n",
        "# Predict using the trained multi-label model\n",
        "pred_sentiment, pred_topic = model.predict(X_new)[0]\n",
        "\n",
        "# Map numeric predictions back to labels\n",
        "sentiment_map = {0: \"Negative\", 1: \"Positive\"}\n",
        "topic_map = {0: \"Health\", 1: \"Tech\"}\n",
        "\n",
        "print(\"Prediction:\")\n",
        "print(\"  Sentiment:\", sentiment_map[pred_sentiment])\n",
        "print(\"  Topic    :\", topic_map[pred_topic])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMZmzJ1oXV3P",
        "outputId": "2463c71c-5dc7-4dad-aa39-bff1298defca"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:\n",
            "  Sentiment: Positive\n",
            "  Topic    : Tech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Data\n",
        "texts = [\n",
        "    \"AI breakthroughs amaze me every day\",\n",
        "    \"This treatment saved my life\",\n",
        "    \"Finance laws are so confusing\",\n",
        "    \"The concert last night was unforgettable\",\n",
        "    \"This robot vacuum is a game changer\",\n",
        "    \"The new fitness routine feels exhausting\",\n",
        "    \"Healthcare policies need urgent reform\",\n",
        "    \"Virtual reality made the game more immersive\",\n",
        "    \"Banking apps should improve security\",\n",
        "    \"This documentary about the cosmos blew my mind\"\n",
        "]\n",
        "\n",
        "# Sentiment (10 classes)\n",
        "sentiments = list(range(10))  # [0–9]\n",
        "\n",
        "# Topic (10 classes)\n",
        "topics = list(range(10))  # [0–9]\n",
        "\n",
        "# 2. Text Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "# 3. One-hot encode the labels\n",
        "y_sentiment = to_categorical(sentiments, num_classes=10)\n",
        "y_topic = to_categorical(topics, num_classes=10)\n",
        "\n",
        "# 4. Train-test split\n",
        "X_train, X_test, y_train_sent, y_test_sent, y_train_topic, y_test_topic = train_test_split(\n",
        "    X, y_sentiment, y_topic, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 5. Build the model\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "inputs = layers.Input(shape=(input_dim,))\n",
        "x = layers.Dense(128, activation='relu')(inputs)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "out_sentiment = layers.Dense(10, activation='softmax', name='sentiment')(x)\n",
        "out_topic = layers.Dense(10, activation='softmax', name='topic')(x)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=[out_sentiment, out_topic])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss={'sentiment': 'categorical_crossentropy', 'topic': 'categorical_crossentropy'},\n",
        "    metrics={'sentiment': 'accuracy', 'topic': 'accuracy'}\n",
        ")\n",
        "\n",
        "# 6. Train\n",
        "model.fit(\n",
        "    X_train,\n",
        "    {'sentiment': y_train_sent, 'topic': y_train_topic},\n",
        "    epochs=40,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 7. Predict\n",
        "pred_sent, pred_topic = model.predict(X_test)\n",
        "pred_sent_labels = np.argmax(pred_sent, axis=1)\n",
        "pred_topic_labels = np.argmax(pred_topic, axis=1)\n",
        "\n",
        "true_sent_labels = np.argmax(y_test_sent, axis=1)\n",
        "true_topic_labels = np.argmax(y_test_topic, axis=1)\n",
        "\n",
        "# 8. Evaluate\n",
        "print(\"\\n=== Sentiment Classification ===\")\n",
        "print(classification_report(true_sent_labels, pred_sent_labels, zero_division=0))\n",
        "\n",
        "print(\"=== Topic Classification ===\")\n",
        "print(classification_report(true_topic_labels, pred_topic_labels, zero_division=0))\n",
        "\n",
        "# 9. Predict new sample\n",
        "new_text = [\"I love the future of robotics and automation\"]\n",
        "print(new_text)\n",
        "X_new = vectorizer.transform(new_text).toarray()\n",
        "new_pred_sent, new_pred_topic = model.predict(X_new)\n",
        "\n",
        "print(\"\\n=== New Prediction ===\")\n",
        "print(\"Sentiment class:\", print(np.argmax(new_pred_sent[0])))\n",
        "print(\"Topic class    :\", np.argmax(new_pred_topic[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMm7YWDsa7jI",
        "outputId": "ec417cf2-4052-4536-b126-3dfba623151a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 4.5961 - sentiment_accuracy: 0.2500 - sentiment_loss: 2.3229 - topic_accuracy: 0.0000e+00 - topic_loss: 2.2732\n",
            "Epoch 2/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 4.5968 - sentiment_accuracy: 0.2500 - sentiment_loss: 2.2936 - topic_accuracy: 0.0000e+00 - topic_loss: 2.3032\n",
            "Epoch 3/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 4.5900 - sentiment_accuracy: 0.1250 - sentiment_loss: 2.3177 - topic_accuracy: 0.1250 - topic_loss: 2.2722\n",
            "Epoch 4/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 4.5396 - sentiment_accuracy: 0.3750 - sentiment_loss: 2.2726 - topic_accuracy: 0.0000e+00 - topic_loss: 2.2670\n",
            "Epoch 5/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 4.5075 - sentiment_accuracy: 0.0000e+00 - sentiment_loss: 2.2718 - topic_accuracy: 0.2500 - topic_loss: 2.2357\n",
            "Epoch 6/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 4.4226 - sentiment_accuracy: 0.3750 - sentiment_loss: 2.2360 - topic_accuracy: 0.3750 - topic_loss: 2.1866\n",
            "Epoch 7/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 4.4633 - sentiment_accuracy: 0.2500 - sentiment_loss: 2.2367 - topic_accuracy: 0.2500 - topic_loss: 2.2266\n",
            "Epoch 8/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 4.4100 - sentiment_accuracy: 0.1250 - sentiment_loss: 2.2194 - topic_accuracy: 0.2500 - topic_loss: 2.1907\n",
            "Epoch 9/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 4.3746 - sentiment_accuracy: 0.5000 - sentiment_loss: 2.1840 - topic_accuracy: 0.5000 - topic_loss: 2.1906\n",
            "Epoch 10/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 4.3464 - sentiment_accuracy: 0.3750 - sentiment_loss: 2.1721 - topic_accuracy: 0.3750 - topic_loss: 2.1743\n",
            "Epoch 11/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 4.3209 - sentiment_accuracy: 0.6250 - sentiment_loss: 2.1572 - topic_accuracy: 0.3750 - topic_loss: 2.1637\n",
            "Epoch 12/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 4.2840 - sentiment_accuracy: 0.5000 - sentiment_loss: 2.1778 - topic_accuracy: 0.7500 - topic_loss: 2.1061\n",
            "Epoch 13/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 4.2462 - sentiment_accuracy: 0.6250 - sentiment_loss: 2.1311 - topic_accuracy: 0.7500 - topic_loss: 2.1151\n",
            "Epoch 14/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 4.1697 - sentiment_accuracy: 0.7500 - sentiment_loss: 2.1132 - topic_accuracy: 0.8750 - topic_loss: 2.0565\n",
            "Epoch 15/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 4.1778 - sentiment_accuracy: 0.6250 - sentiment_loss: 2.1039 - topic_accuracy: 0.8750 - topic_loss: 2.0739\n",
            "Epoch 16/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 4.2716 - sentiment_accuracy: 0.5000 - sentiment_loss: 2.1593 - topic_accuracy: 0.6250 - topic_loss: 2.1123\n",
            "Epoch 17/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 4.2265 - sentiment_accuracy: 0.6250 - sentiment_loss: 2.1337 - topic_accuracy: 0.6250 - topic_loss: 2.0927\n",
            "Epoch 18/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 4.1921 - sentiment_accuracy: 0.6250 - sentiment_loss: 2.1239 - topic_accuracy: 0.7500 - topic_loss: 2.0682\n",
            "Epoch 19/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 4.0511 - sentiment_accuracy: 0.5000 - sentiment_loss: 2.0661 - topic_accuracy: 1.0000 - topic_loss: 1.9850\n",
            "Epoch 20/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 4.0632 - sentiment_accuracy: 0.6250 - sentiment_loss: 2.0592 - topic_accuracy: 0.7500 - topic_loss: 2.0040\n",
            "Epoch 21/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 4.0457 - sentiment_accuracy: 0.7500 - sentiment_loss: 2.0322 - topic_accuracy: 1.0000 - topic_loss: 2.0135\n",
            "Epoch 22/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 4.0530 - sentiment_accuracy: 0.8750 - sentiment_loss: 2.0554 - topic_accuracy: 0.8750 - topic_loss: 1.9977\n",
            "Epoch 23/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 3.9873 - sentiment_accuracy: 0.7500 - sentiment_loss: 2.0283 - topic_accuracy: 0.8750 - topic_loss: 1.9590\n",
            "Epoch 24/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 3.9867 - sentiment_accuracy: 0.6250 - sentiment_loss: 2.0147 - topic_accuracy: 1.0000 - topic_loss: 1.9720\n",
            "Epoch 25/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 3.9045 - sentiment_accuracy: 0.8750 - sentiment_loss: 1.9665 - topic_accuracy: 1.0000 - topic_loss: 1.9379\n",
            "Epoch 26/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 3.8938 - sentiment_accuracy: 0.6250 - sentiment_loss: 2.0067 - topic_accuracy: 1.0000 - topic_loss: 1.8871\n",
            "Epoch 27/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.8700 - sentiment_accuracy: 0.7500 - sentiment_loss: 1.9807 - topic_accuracy: 1.0000 - topic_loss: 1.8892\n",
            "Epoch 28/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 3.8218 - sentiment_accuracy: 1.0000 - sentiment_loss: 1.9020 - topic_accuracy: 1.0000 - topic_loss: 1.9198\n",
            "Epoch 29/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 3.8872 - sentiment_accuracy: 0.6250 - sentiment_loss: 1.9743 - topic_accuracy: 1.0000 - topic_loss: 1.9129\n",
            "Epoch 30/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 3.8708 - sentiment_accuracy: 0.7500 - sentiment_loss: 1.9650 - topic_accuracy: 1.0000 - topic_loss: 1.9059\n",
            "Epoch 31/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.6970 - sentiment_accuracy: 0.8750 - sentiment_loss: 1.8902 - topic_accuracy: 1.0000 - topic_loss: 1.8069\n",
            "Epoch 32/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 3.7961 - sentiment_accuracy: 1.0000 - sentiment_loss: 1.9158 - topic_accuracy: 1.0000 - topic_loss: 1.8803\n",
            "Epoch 33/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 3.6126 - sentiment_accuracy: 1.0000 - sentiment_loss: 1.8095 - topic_accuracy: 1.0000 - topic_loss: 1.8031\n",
            "Epoch 34/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 3.6615 - sentiment_accuracy: 1.0000 - sentiment_loss: 1.8882 - topic_accuracy: 1.0000 - topic_loss: 1.7734\n",
            "Epoch 35/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 3.5473 - sentiment_accuracy: 0.8750 - sentiment_loss: 1.7924 - topic_accuracy: 1.0000 - topic_loss: 1.7549\n",
            "Epoch 36/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 3.5889 - sentiment_accuracy: 1.0000 - sentiment_loss: 1.8123 - topic_accuracy: 1.0000 - topic_loss: 1.7765\n",
            "Epoch 37/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 3.6757 - sentiment_accuracy: 1.0000 - sentiment_loss: 1.8254 - topic_accuracy: 1.0000 - topic_loss: 1.8503\n",
            "Epoch 38/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 3.4663 - sentiment_accuracy: 1.0000 - sentiment_loss: 1.7325 - topic_accuracy: 1.0000 - topic_loss: 1.7338\n",
            "Epoch 39/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.4040 - sentiment_accuracy: 1.0000 - sentiment_loss: 1.7240 - topic_accuracy: 1.0000 - topic_loss: 1.6800\n",
            "Epoch 40/40\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 3.5710 - sentiment_accuracy: 0.8750 - sentiment_loss: 1.8158 - topic_accuracy: 1.0000 - topic_loss: 1.7552\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "=== Sentiment Classification ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       0.0\n",
            "           1       0.00      0.00      0.00       1.0\n",
            "           8       0.00      0.00      0.00       1.0\n",
            "           9       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00       2.0\n",
            "   macro avg       0.00      0.00      0.00       2.0\n",
            "weighted avg       0.00      0.00      0.00       2.0\n",
            "\n",
            "=== Topic Classification ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00       1.0\n",
            "           6       0.00      0.00      0.00       0.0\n",
            "           8       0.00      0.00      0.00       1.0\n",
            "           9       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00       2.0\n",
            "   macro avg       0.00      0.00      0.00       2.0\n",
            "weighted avg       0.00      0.00      0.00       2.0\n",
            "\n",
            "['I love the future of robotics and automation']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\n",
            "=== New Prediction ===\n",
            "9\n",
            "Sentiment class: None\n",
            "Topic class    : 3\n"
          ]
        }
      ]
    }
  ]
}